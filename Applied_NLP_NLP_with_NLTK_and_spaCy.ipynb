{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP98pd9Ye1BOezv/Rxm5G9Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MhAmine/intro-to-NLP-Workshop/blob/main/Applied_NLP_NLP_with_NLTK_and_spaCy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Natural Language Processing (NLP) enables machines to understand and process human language. Python has two powerful NLP libraries:\n",
        "\n",
        "NLTK (Natural Language Toolkit): Best for linguistic processing, tokenization, stemming, etc.\n",
        "spaCy: Fast, efficient, and best for production-level NLP tasks like Named Entity Recognition (NER), dependency parsing, etc."
      ],
      "metadata": {
        "id": "nO4OJ_8lBiFV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll explore key NLP tasks in English and French using NLTK and spaCy."
      ],
      "metadata": {
        "id": "FCFkTQL0Bvp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qWh5cUSrBycJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  1. Installing Required Libraries"
      ],
      "metadata": {
        "id": "YJKqe1qKByjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install nltk spacy\n"
      ],
      "metadata": {
        "id": "AS9OMohQBz-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "8HEtynPcCHav"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### # ✨ Note : For French NLP with spaCy, download the French language model:"
      ],
      "metadata": {
        "id": "XLBEV2x0B294"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download fr_core_news_sm\n",
        "nlp = spacy.load(\"fr_core_news_sm\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ReHxwFMwB8qb",
        "outputId": "d3f31f45-a370-4bcd-b1cc-7fa103f207d1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fr-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.7.0/fr_core_news_sm-3.7.0-py3-none-any.whl (16.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from fr-core-news-sm==3.7.0) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2025.1.31)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### # ✨ 2. Tokenization (Splitting Text into Words or Sentences)\n",
        "\n",
        "🔹 Using spaCy"
      ],
      "metadata": {
        "id": "as3kLr9yDPKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp_en = spacy.load(\"en_core_web_sm\")\n",
        "nlp_fr = spacy.load(\"fr_core_news_sm\")\n",
        "\n",
        "doc_en = nlp_en(text_en)\n",
        "doc_fr = nlp_fr(text_fr)\n",
        "\n",
        "tokens_en_spacy = [token.text for token in doc_en]\n",
        "tokens_fr_spacy = [token.text for token in doc_fr]\n",
        "\n",
        "print(\"spaCy English Tokens:\", tokens_en_spacy)\n",
        "print(\"spaCy French Tokens:\", tokens_fr_spacy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnDzVR10DT6n",
        "outputId": "85911580-f2a5-480d-d499-eb110cdd032b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spaCy English Tokens: ['Natural', 'Language', 'Processing', 'is', 'amazing', '!', 'Let', \"'s\", 'learn', 'it', '.']\n",
            "spaCy French Tokens: ['Le', 'traitement', 'du', 'langage', 'naturel', 'est', 'fascinant', '!', 'Apprenons', '-', 'le', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### # ✨ 3. Stopwords Removal (Common Words Filtering)\n",
        "\n",
        "-  Using spaCy:  SpaCy is more efficient as it detects stopwords automatically."
      ],
      "metadata": {
        "id": "tFkHx8p9EunX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_en_spacy = [token.text for token in doc_en if not token.is_stop]\n",
        "filtered_fr_spacy = [token.text for token in doc_fr if not token.is_stop]\n",
        "\n",
        "print(\"Filtered spaCy English Tokens:\", filtered_en_spacy)\n",
        "print(\"Filtered spaCy French Tokens:\", filtered_fr_spacy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ldnr7NuDEz4A",
        "outputId": "a8e4257c-6a93-412a-fc12-c8bff0a858be"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered spaCy English Tokens: ['Natural', 'Language', 'Processing', 'amazing', '!', 'Let', 'learn', '.']\n",
            "Filtered spaCy French Tokens: ['traitement', 'langage', 'naturel', 'fascinant', '!', 'Apprenons', '-', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### # ✨ 4. Stemming & Lemmatization (Finding Word Roots)\n",
        "- Using spaCy (Lemmatization)"
      ],
      "metadata": {
        "id": "Zj48Gkc0FILg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmas_en = [token.lemma_ for token in doc_en]\n",
        "lemmas_fr = [token.lemma_ for token in doc_fr]\n",
        "\n",
        "print(\"Lemmatized English Tokens:\", lemmas_en)\n",
        "print(\"Lemmatized French Tokens:\", lemmas_fr)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ovO3vzMFOzM",
        "outputId": "354823f4-dbc2-432d-827e-1cdd92f1c744"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized English Tokens: ['Natural', 'Language', 'Processing', 'be', 'amazing', '!', 'let', 'us', 'learn', 'it', '.']\n",
            "Lemmatized French Tokens: ['le', 'traitement', 'de', 'langage', 'naturel', 'être', 'fasciner', '!', 'apprendre', '-', 'le', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### # ✨ 5. Part-of-Speech (POS) Tagging\n",
        "- Identifying the grammatical role of words."
      ],
      "metadata": {
        "id": "i3yxDzpFFbqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tags_en_spacy = [(token.text, token.pos_) for token in doc_en]\n",
        "pos_tags_fr_spacy = [(token.text, token.pos_) for token in doc_fr]\n",
        "\n",
        "print(\"spaCy POS Tags (English):\", pos_tags_en_spacy)\n",
        "print(\"spaCy POS Tags (French):\", pos_tags_fr_spacy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mD8RxkRiFcxH",
        "outputId": "d658127c-7b6e-4e39-e373-ede9e90eeb91"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spaCy POS Tags (English): [('Natural', 'PROPN'), ('Language', 'PROPN'), ('Processing', 'PROPN'), ('is', 'AUX'), ('amazing', 'ADJ'), ('!', 'PUNCT'), ('Let', 'VERB'), (\"'s\", 'PRON'), ('learn', 'VERB'), ('it', 'PRON'), ('.', 'PUNCT')]\n",
            "spaCy POS Tags (French): [('Le', 'DET'), ('traitement', 'NOUN'), ('du', 'ADP'), ('langage', 'NOUN'), ('naturel', 'ADJ'), ('est', 'AUX'), ('fascinant', 'VERB'), ('!', 'PUNCT'), ('Apprenons', 'PRON'), ('-', 'PROPN'), ('le', 'PROPN'), ('.', 'PUNCT')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✨ 6. Named Entity Recognition (NER)\n",
        "- Extracting names, dates, locations, etc."
      ],
      "metadata": {
        "id": "UuYSmhY3FrRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_en = \"Elon Musk founded SpaceX in 2002 in the United States.\"\n",
        "text_fr = \"Emmanuel Macron est le président de la France depuis 2017.\"\n",
        "\n",
        "doc_en = nlp_en(text_en)\n",
        "doc_fr = nlp_fr(text_fr)\n",
        "\n",
        "print(\"English Entities:\")\n",
        "for ent in doc_en.ents:\n",
        "    print(ent.text, \"→\", ent.label_)\n",
        "\n",
        "print(\"\\nFrench Entities:\")\n",
        "for ent in doc_fr.ents:\n",
        "    print(ent.text, \"→\", ent.label_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6apN0K8GF9E",
        "outputId": "023344f5-6345-4bc9-ab8c-5e7508091853"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English Entities:\n",
            "Elon Musk → PERSON\n",
            "2002 → DATE\n",
            "the United States → GPE\n",
            "\n",
            "French Entities:\n",
            "Emmanuel Macron → PER\n",
            "la France → LOC\n"
          ]
        }
      ]
    }
  ]
}